{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd155408",
   "metadata": {},
   "source": [
    "# <center> Deep Learning Project:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a954f1cd",
   "metadata": {},
   "source": [
    "> - __Ahmed Abdelazeem__ (m20210433)\n",
    "> - __Omar Jarir__ (m20201378)  \n",
    "> - __Chung-Ting Huang__ (m20210437) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680a8114",
   "metadata": {},
   "source": [
    "- The objective of this project is to forecast the evolutions of \"bitcoin\" prices, by using reccurent neural networks.\n",
    "such as RNN and LSTM.\n",
    "- The project is organized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fcad8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install keras-tuner --upgrade\n",
    "# !pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3facbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d3f3c4",
   "metadata": {},
   "source": [
    "- __Importing the necessary libraries ðŸ“š:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7afd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import random as python_random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be2fc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, BatchNormalization, LSTM, Embedding, Input, Dropout, SimpleRNN, RNN, Bidirectional\n",
    "#from keras.Optimizer import Adam, SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl, schedules\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, History, ReduceLROnPlateau \n",
    "#from keras.utils import plot_model\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor \n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41169d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "import scikitplot as skplt\n",
    "import xgboost\n",
    "\n",
    "from mlxtend.evaluate import mcnemar\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from yellowbrick.model_selection import RFECV\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52987b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b710363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEED = 2022\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "time_steps=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6defba",
   "metadata": {},
   "source": [
    "Fixing the random number seed to ensure our results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b17cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session = K.get_session()\n",
    "# init_op = tf.group(tf.tables_initializer(),tf.global_variables_initializer(),\n",
    "# tf.local_variables_initializer())\n",
    "#session.run(init_op)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "python_random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1d56c",
   "metadata": {},
   "source": [
    "- __Helper functions:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a0c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to create dataframe with metrics\n",
    "\n",
    "def performanceMetricsDF(metricsObj, yTrain, yPredTrain, yTest, yPredTest):\n",
    "    measures_list = ['ACCURACY','PRECISION', 'RECALL','F1 SCORE','AUC']\n",
    "    train_results = [metricsObj.accuracy_score(yTrain, yPredTrain),\n",
    "                metricsObj.precision_score(yTrain, yPredTrain),\n",
    "                metricsObj.recall_score(yTrain, yPredTrain, average='weighted'),\n",
    "                metricsObj.f1_score(yTrain, yPredTrain, average='weighted'),\n",
    "                metricsObj.roc_auc_score(yTrain, yPredTrain),    \n",
    "                ]\n",
    "    test_results = [metricsObj.accuracy_score(yTest, yPredTest),\n",
    "               metricsObj.precision_score(yTest, yPredTest),\n",
    "               metricsObj.recall_score(yTest, yPredTest, average='weighted'),\n",
    "               metricsObj.f1_score(yTest, yPredTest, average='weighted'),\n",
    "               metricsObj.roc_auc_score(yTest, yPredTest), \n",
    "               ]\n",
    "    resultsDF = pd.DataFrame({'Measure': measures_list, 'Train': train_results, 'Test':test_results})\n",
    "    return(resultsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221a8c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix - Adapted from https://github.com/DTrimarchi10/confusion_matrix/blob/master/cf_matrix.py\n",
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                   \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.heatmap(cf,annot=box_labels, fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09be78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to find outliers on columns based on percentile\n",
    "def removeOutliers(df, colList, lowPercentile=0.05, highPercentile=0.95, verbose=False):\n",
    "    quant_df = df[colList].quantile([lowPercentile, highPercentile])\n",
    "    if verbose:\n",
    "        print(quant_df)\n",
    "    for name in list(df[colList].columns):\n",
    "        df = df[(df[name] >= quant_df.loc[lowPercentile, name]) & (df[name] <= quant_df.loc[highPercentile, name])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c74269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(nl=1, nn=256):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(2,), activation=\"relu\"))\n",
    "    # Add as many hidden layers as specified in nl\n",
    "    for i in range(nl):\n",
    "        # Layers have nn neurons\n",
    "        model.add(Dense(nn, activation='relu'))\n",
    "    # End defining and compiling your model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64253c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_raw_visualization(data):\n",
    "    time_data = data[date_time_key]\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    for i in range(len(feature_keys)):\n",
    "        key = feature_keys[i]\n",
    "        c = colors[i % (len(colors))]\n",
    "        t_data = data[key]\n",
    "        t_data.index = time_data\n",
    "        t_data.head()\n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i // 2, i % 2],\n",
    "            color=c,\n",
    "            title=\"{} - {}\".format(titles[i], key),\n",
    "            rot=25,\n",
    "        )\n",
    "        ax.legend([titles[i]])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb99cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_heatmap(data):\n",
    "    plt.matshow(data.corr())\n",
    "    plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n",
    "\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ab3cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function plot the \"loss\" and the \"val_loss\" variables.\n",
    "def visualize_loss(history, title):\n",
    "    \"\"\"\n",
    "    history: history of the model.\n",
    "    title: Title of the plot.\n",
    "    \"\"\"\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(1, len(loss)+1)\n",
    "    plt.plot(epochs, loss, \"g\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02379c28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function plot the \"accuracy\" and the \"val_accuracy\" variables.\n",
    "def visualize_accuracy(history, title):\n",
    "    \"\"\"\n",
    "    history: history of the model.\n",
    "    title: Title of the plot.\n",
    "    \"\"\"\n",
    "    loss = history.history[\"accuracy\"]\n",
    "    val_loss = history.history[\"val_accuracy\"]\n",
    "    epochs = range(1, len(loss)+1)\n",
    "    plt.plot(epochs, loss, \"g\", label=\"Training accuracy\")\n",
    "    plt.plot(epochs, val_loss, \"b\", label=\"Validation accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185d55e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function plot the \"error\" and the \"val_error\" variables.\n",
    "def visualize_error(history, title):\n",
    "    \"\"\"\n",
    "    history: history of the model.\n",
    "    title: Title of the plot.\n",
    "    \"\"\"\n",
    "    error = history.history[\"mae\"]\n",
    "    val_error = history.history[\"val_mae\"]\n",
    "    epochs = range(len(error))\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(epochs, error, \"b\", label=\"Training error\")\n",
    "    plt.plot(epochs, val_error, \"r\", label=\"Validation error\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d3c87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    \"\"\"\n",
    "    plot_data:\n",
    "    delta:\n",
    "    title:\n",
    "    \"\"\"\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b92b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_predictions(val, predicted, title, date):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.plot(date, val, color='blue',label='Actual') # date, \n",
    "    plt.plot(date,predicted, alpha=0.7, color='red',label='Predict') # date, \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Something')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c87322",
   "metadata": {},
   "source": [
    "- __Loading the dataset:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c93f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds= pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589e519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63537575",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the shape of the dataset:\n",
    "\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6a7c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the target variable.\n",
    "ds[\"Bankrupt?\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d10a6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing the model:\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=10, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc1a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# I get to select 17 features.\n",
    "\n",
    "# Applying RFECV (Recursive Feature Elimination) to select features\n",
    "# see https://www.scikit-yb.org/en/latest/api/model_selection/rfecv.html\n",
    "cv = StratifiedKFold(3)\n",
    "vis = RFECV(rf_model, scoring='f1_weighted') ## , cv=cv, we dont need to use cross validation\n",
    "vis.fit(ds.drop(columns=\"Bankrupt?\"), ds[\"Bankrupt?\"])\n",
    "vis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf1b942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed380da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Features to include: \n",
    "ds.columns[vis.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cec36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be the the new data set.\n",
    "\n",
    "ds[ds.columns[vis_support_]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57220333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14b9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking that the target distribution is imbalanced.\n",
    "\n",
    "sns.countplot(y='Bankrupt?', data = ds, palette='viridis', orient = 'h')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de312ac0",
   "metadata": {},
   "source": [
    "# Data Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752ed97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357dfcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can see that our dataset does not contain any duplicates.\n",
    "\n",
    "ds.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114c1de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the type of the columns\n",
    "ds.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e66007a",
   "metadata": {},
   "source": [
    "# Data Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1822409",
   "metadata": {},
   "source": [
    "## Dealing with outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365126f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = removeOutliers(X, colList=cols , lowPercentile=0.05, highPercentile=0.95, verbose=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f8d64",
   "metadata": {},
   "source": [
    "## One Hot encoding the categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1bc7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transforming a list of columns to categorical.\n",
    "# I need to get the categorical features:\n",
    "\n",
    "cols = [\"\", \"\", \"\"]\n",
    "ds[cols] = ds[cols].apply(lambda x:x.astype(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7881e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encode categorical values into dummy variables.\n",
    "\n",
    "cols = [\"\", \"\", \"\"]\n",
    "ce_one_hot = ce.OneHotEncoder(cols = cols, use_cat_names=True)\n",
    "X = ce_one_hot.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1190a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = X['']\n",
    "X = X.drop(columns=['Bankrupt?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557cd9e9",
   "metadata": {},
   "source": [
    "## Splitting the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f97d7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset intro train and test sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,\n",
    "                                   shuffle =True, stratify=y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc946f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb6579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb34c042",
   "metadata": {},
   "source": [
    "## Data Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199499da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We use the standard scaler in order to normalize the data:\n",
    "\n",
    "sc = StandardScaler() \n",
    "X_train = sc.fit_transform(X_train) \n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172051c1",
   "metadata": {},
   "source": [
    "# Modelling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e692acf",
   "metadata": {},
   "source": [
    "## Building The First Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    FalseNegatives(name='fn'), BinaryAccuracy(name='accuracy'),\n",
    "    Precision(name='precision'), Recall(name='recall'), AUC(name='auc'),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c565be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building the first model\n",
    "\n",
    "def create_model(nn=16, nl=1, learning_rate=0.001, activation=\"relu\", optim=\"adam\", \n",
    "                 init=\"glorot_uniform\", loss = \"binary_crossentropy\"):\n",
    "    classifier = Sequential()\n",
    "    # Let's add the first hidden layers\n",
    "    classifier.add(Dense(units=nn, kernel_initializer=init, activation=activation,\n",
    "                         input_dim=X_train.shape[1])) \n",
    "    \n",
    "    for _ in range(nl):\n",
    "        classifier.add(Dense(units=8, kernel_initializer=init, activation=activation))\n",
    "        classifier.add(Dense(units=1, kernel_initializer=init, activation=\"sigmoid\"))\n",
    "    \n",
    "    classifier.compile(optimizer=optim, loss=loss, metrics=[\"accuracy\"])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a94ca7",
   "metadata": {},
   "source": [
    "### Hyper parameters tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e0546",
   "metadata": {},
   "source": [
    "#### Using Keras tuner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is similar to the one above:\n",
    "\n",
    "def get_model_design_hp(hp, nl=1):\n",
    "    \n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units=hp.Int('First Layer',16,32,step=8,default=16),\n",
    "                              kernel_initializer=\"glorot_uniform\", activation=\"relu\",\n",
    "                         input_dim=X_train.shape[1])) \n",
    "         \n",
    "    for _ in range(nl):\n",
    "        classifier.add(Dense(units=hp.Int('First Layer',4,8,step=4,default=4), kernel_initializer=\"glorot_uniform\", \n",
    "                             activation=\"relu\"))\n",
    "        classifier.add(Dense(units=1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "            \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "                loss='binary_crossentropy',metrics= METRICS) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18df4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does this use cross validation or not?\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    get_model_design_hp,\n",
    "    objective= 'val_loss' ,\n",
    "    max_epochs=100,\n",
    "    hyperband_iterations=2,\n",
    "    directory = LOG_DIR    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train\n",
    "             validation_data = (X_test, y_test),\n",
    "             epochs=2,\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56446821",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7082f0",
   "metadata": {},
   "source": [
    "#### Using Scikit Learn wrapper:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9e7b0",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe316b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras_classifier = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "# epochs=50, batch_size=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd3753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "# evaluate using 10-fold cross validation\n",
    "\n",
    "#kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "#results = cross_val_score(keras_classifier, X_train, y_train, cv=kfold)\n",
    "#print(results.mean())\n",
    "#print(results.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03b5dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [50, 100, 150]\n",
    "batches = [64, 128, 256]\n",
    "\n",
    "param_grid = dict(optim=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=keras_classifier, param_distributions=param_grid)\n",
    "\n",
    "random_search_results = grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (random_search_results.best_score_, random_search_results.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62b89e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means = random_search_results.cv_results_['mean_test_score']\n",
    "stds = random_search_results.cv_results_['std_test_score']\n",
    "params = random_search_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04826e7",
   "metadata": {},
   "source": [
    "#### Building the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ea2ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now that we have the best hyper parameters we can build the model:\n",
    "\n",
    "# classifier = create_model(nn=16, here i should include all the found parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95422cf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52577f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Path + name of the file we wanna create.\n",
    "path_checkpoint = \"First_model.h5\"\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"auto\")\n",
    "\n",
    "model_save = ModelCheckpoint(monitor=\"val_loss\", filepath=path_checkpoint, verbose=1, \n",
    "                             save_best_only=True)\n",
    "\n",
    "Hist_first = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1155b2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = classifier.fit(X_train, y_train,  \n",
    "                      batch_size=64,\n",
    "                      validation_split=0.2,\n",
    "                      # the number of epochs can be high, early stopping will stop training when the val_loss is not improving\n",
    "                      epochs=1000, \n",
    "                      callbacks = [early_stopping, model_save, Hist_first])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbad24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_train = classifier.predict(X_train)\n",
    "y_pred_test = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abec128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting the predictions to the right format:\n",
    "\n",
    "y_pred_train=(y_pred_train>0.5).astype(int)\n",
    "y_pred_test=(y_pred_test>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc5608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Showing the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test,y_pred_test)\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['0', '1']\n",
    "make_confusion_matrix(cm, group_names=labels, categories=categories, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8533a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Showing the classification report:\n",
    "\n",
    "target_names=[\"No Default\", \"Default\"]\n",
    "print(classification_report(y_test, y_pred_test, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc56301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Showing the results performance.\n",
    "\n",
    "resultsDF = performanceMetricsDF(metrics, y_train, y_pred_train, \n",
    "                                 y_test, y_pred_test)\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7b2ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the train and validation loss:\n",
    "\n",
    "visualize_loss(hist, \"Training and validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fccfca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the train and validation accuracy:\n",
    "\n",
    "visualize_accuracy(hist, \"Training and validation accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe i can also add other plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27ee66",
   "metadata": {},
   "source": [
    "## Using class_weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the weights of each class\n",
    "\n",
    "class_weights=dict(enumerate(class_weight.compute_class_weight('balanced', \n",
    "                            classes=np.unique(y_train), y=y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fbd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the best hyper parameters we can build the model:\n",
    "\n",
    "# classifier = create_model(nn=16, here i should include all the found parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path + name of the file we wanna create.\n",
    "path_checkpoint = \"Second_model.h5\"\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"auto\")\n",
    "\n",
    "model_save = ModelCheckpoint(monitor=\"val_loss\", filepath=path_checkpoint, verbose=1, \n",
    "                             save_best_only=True)\n",
    "\n",
    "Hist_second = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I SHOULD PAY ATTENTION TO THE MODEL NAME.\n",
    "\n",
    "hist = classifier.fit(X_train, y_train,  \n",
    "                      batch_size=64,\n",
    "                      validation_split=0.2,\n",
    "                      epochs=1000,\n",
    "                      callbacks = [early_stopping, model_save, Hist_second],  \n",
    "                      class_weight=class_weights\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bbb00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = classifier.predict(X_train)\n",
    "y_pred_test = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the predictions to the right format:\n",
    "\n",
    "y_pred_train=(y_pred_train>0.5).astype(int)\n",
    "y_pred_test=(y_pred_test>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e2b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the confusion matrix\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test,y_pred_test)\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['0', '1']\n",
    "make_confusion_matrix(cm, group_names=labels, categories=categories, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the classification report:\n",
    "\n",
    "target_names=[\"No Default\", \"Default\"]\n",
    "print(classification_report(y_test, y_pred_test, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab72757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the results performance.\n",
    "\n",
    "resultsDF = performanceMetricsDF(metrics, y_train, y_pred_train, \n",
    "                                 y_test, y_pred_test)\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the train and validation loss:\n",
    "\n",
    "visualize_loss(hist, \"Training and validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ed0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the train and validation accuracy:\n",
    "\n",
    "visualize_accuracy(hist, \"Training and validation accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdab4f3",
   "metadata": {},
   "source": [
    "## Using a Focal Loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041a17a",
   "metadata": {},
   "source": [
    "https://github.com/umbertogriffo/focal-loss-keras\n",
    "- I should add few comments about the focal loss function, and why we choose to use it.\n",
    "- \"focal loss down-weights the well-classified examples. This has the net effect of putting more training emphasis on that data that is hard to classify. In a practical setting where we have a data imbalance, our majority class will quickly become well-classified since we have much more data for it. Thus, in order to insure that we also achieve high accuracy on our minority class, we can use the focal loss to give those minority class examples more relative weight during training.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e097a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=4.):\n",
    "\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"Focal loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is probability after softmax\n",
    "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "        model_out = tf.add(y_pred, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "        weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
    "        fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        return tf.reduce_mean(reduced_fl)\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af2589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAKE SURE YOU CHOOSE THE RIGHT PARAMETERS, I NEED TO CHECK THAT OLD FILE AND SEE IF ITS THE SAME OR NOT.\n",
    "\n",
    "create_model(loss = focal_loss(gamma=1.5,alpha=0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba97f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Path + name of the file we wanna create.\n",
    "path_checkpoint = \"Third_model.h5\"\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"auto\")\n",
    "\n",
    "model_save = ModelCheckpoint(monitor=\"val_loss\", filepath=path_checkpoint, verbose=1, \n",
    "                             save_best_only=True)\n",
    "\n",
    "Hist_third = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64afbe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I SHOULD PAY ATTENTION TO THE MODEL NAME.\n",
    "\n",
    "hist = classifier.fit(X_train, y_train,  \n",
    "                      batch_size=64,\n",
    "                      validation_split=0.2,\n",
    "                      epochs=1000,\n",
    "                      callbacks = [early_stopping, model_save, Hist_third], \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab3796",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = classifier.predict(X_train)\n",
    "y_pred_test = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b40fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the predictions to the right format:\n",
    "\n",
    "y_pred_train=(y_pred_train>0.5).astype(int)\n",
    "y_pred_test=(y_pred_test>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a467133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the confusion matrix\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test,y_pred_test)\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['0', '1']\n",
    "make_confusion_matrix(cm, group_names=labels, categories=categories, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the classification report:\n",
    "\n",
    "target_names=[\"No Default\", \"Default\"]\n",
    "print(classification_report(y_test, y_pred_test, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e138b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the results performance.\n",
    "\n",
    "resultsDF = performanceMetricsDF(metrics, y_train, y_pred_train, \n",
    "                                 y_test, y_pred_test)\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8079f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the train and validation loss:\n",
    "\n",
    "visualize_loss(hist, \"Training and validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the train and validation accuracy:\n",
    "\n",
    "visualize_accuracy(hist, \"Training and validation accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731e1de",
   "metadata": {},
   "source": [
    "## Using S.M.O.T.E:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a6972",
   "metadata": {},
   "source": [
    "- Should insert a small introduction about what S.M.O.T.E is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be74e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# S.M.O.T.E\n",
    "\n",
    "sm = SMOTE(k_neighbors=10, random_state=SEED)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7960a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAKE SURE YOU CHOOSE THE RIGHT PARAMETERS, I NEED TO CHECK THAT OLD FILE AND SEE IF ITS THE SAME OR NOT.\n",
    "\n",
    "# create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f344f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path + name of the file we wanna create.\n",
    "path_checkpoint = \"Fourth_model.h5\"\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"auto\")\n",
    "\n",
    "model_save = ModelCheckpoint(monitor=\"val_loss\", filepath=path_checkpoint, verbose=1, \n",
    "                             save_best_only=True)\n",
    "\n",
    "Hist_fourth = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I SHOULD PAY ATTENTION TO THE MODEL NAME.\n",
    "\n",
    "hist = classifier.fit(X_train, y_train,  \n",
    "                      batch_size=64,\n",
    "                      validation_split=0.2,\n",
    "                      epochs=1000,\n",
    "                      callbacks = [early_stopping, model_save, Hist_third], \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = classifier.predict(X_train)\n",
    "y_pred_test = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the predictions to the right format:\n",
    "\n",
    "y_pred_train=(y_pred_train>0.5).astype(int)\n",
    "y_pred_test=(y_pred_test>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770657f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the confusion matrix\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test,y_pred_test)\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['0', '1']\n",
    "make_confusion_matrix(cm, group_names=labels, categories=categories, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the classification report:\n",
    "\n",
    "target_names=[\"No Default\", \"Default\"]\n",
    "print(classification_report(y_test, y_pred_test, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the results performance.\n",
    "\n",
    "resultsDF = performanceMetricsDF(metrics, y_train, y_pred_train, \n",
    "                                 y_test, y_pred_test)\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the train and validation loss:\n",
    "\n",
    "visualize_loss(hist, \"Training and validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad669dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the train and validation accuracy:\n",
    "\n",
    "visualize_accuracy(hist, \"Training and validation accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0cb52",
   "metadata": {},
   "source": [
    "## Results discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b1177",
   "metadata": {},
   "source": [
    "- Insert results discussion here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff69c83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2 = time.perf_counter()\n",
    "print('time taken to run:',(t2-t1)/60.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
